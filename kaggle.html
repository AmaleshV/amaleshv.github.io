<!DOCTYPE html>
<html lang="en">
<head>
	
	<!-- Basic -->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<title>Amalesh - Blog Post</title>
	<meta name="description" content="">
	<meta name="keywords" content="">
	
	<!-- Mobile Specific Metas -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
	
	<!-- Load Fonts -->
	<link href='https://fonts.googleapis.com/css?family=Roboto+Mono:400,100,300italic,300,100italic,400italic,500,500italic,700,700italic&amp;subset=latin,cyrillic' rel='stylesheet'>
	
	<!-- CSS -->
	<link rel="stylesheet" href="css/glitche-basic.css" />
	<link rel="stylesheet" href="css/glitche-layout.css" />
	<link rel="stylesheet" href="css/ionicons.css" />
	<link rel="stylesheet" href="css/magnific-popup.css" />
	<link rel="stylesheet" href="css/animate.css" />
	
	<!--[if lt IE 9]>
	<script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
	
	<!-- Favicons -->
	<link rel="shortcut icon" href="images/favicons/favicon.ico">
	<link rel="apple-touch-icon" href="images/favicons/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/favicons/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/favicons/apple-touch-icon-114x114.png">
	
</head>

<body>	
	
	<!-- Preloader -->
	<div class="preloader">
		<div class="centrize full-width">
			<div class="vertical-center">
				<div class="pre-inner">
					<div class="load typing-load"><p>loading...</p></div>
					<span class="typed-load"></span>
				</div>
			</div>
		</div>
	</div>
	
	<!-- Container -->
	<div class="container">
		
		<!-- Header -->
		<header>
			<div class="head-top">
				<a href="#" class="menu-btn"><span></span></a>
				<div class="top-menu">
					<ul>
						<li><a href="resume.html" class="lnk">Resume</a></li>
						<li><a href="portfolio.html" class="lnk">Portfolio</a></li>
						<li class="active"><a href="blog.html" class="lnk">Blog</a></li>
						<li><a href="contacts.html" class="btn">Contacts</a></li>
					</ul>
				</div>
			</div>
		</header>
		
		<!-- Wrapper -->
		<div class="wrapper">
		
			<!-- Started -->
			<div class="section started">
				<div class="centrize full-width">
					<div class="vertical-center">
						<div class="started-content">
							<div class="date">07 Jan 2020</div>
							<div class="h-title blog_title">Automate Kaggle Competition with the help of Google Colab</div>
							<div class="h-subtitle typing-bread">
								<p><a href="index.html">Home</a> / <a href="blog.html">Blog</a></p>
							</div>
							<span class="typed-bread"></span>
						</div>
					</div>
				</div>
				<a href="#" class="mouse_btn"><span class="ion ion-mouse"></span></a>
			</div>
			
			<!-- Blog -->
			<div class="section blog">
				<div class="content">
					<div class="single-post-text">
						<p>When I started working on Kaggle problems, I was stressed working on Kaggle Notebooks. Having worked in Spyder and Jupyter notebooks, I was not comfortable working in Kaggle. In the process of figuring out few utilities like increasing RAM, loading data through API, Use of GPU, etc, I found Colab solutions more readily available (perhaps it’s a Google thing!). I figured out a few simple tricks in Colab to make working with Colab easier and create a Kaggle ML pipeline to automate the process.
						</p>
						<p><img class="center" src="images/works/kaggle_1060x680.jpeg" alt="" /></p>
						<p>Google Colab is a free Jupyter/IPython notebook environment that requires no setup and runs entirely in the cloud.</p>
						<p><strong>
							How to get the Best out of Colab?</strong><br>
							After creating a new Python Notebook, make sure to change the runtime type to GPU and you’ll be allocated ~12.72 GB RAM and NVIDIA Tesla P4 or NVIDIA Tesla K80 or NVIDIA Tesla P100 GPU ~7.1 GB as GPU. You can increase the memory allocation to ~25.51 GB RAM and GPU to ~ 16 GB by running the following command.
						</p>

						<kbd><b>
							d=[]
							<br>while(1):<br>
							&nbsp;d.append('1')</b>
							</kbd>
						<p>The above command tries to expand (thanks to ‘append’ command) RAM and crashes in the process. Click on the ‘get more ram’ after the crash of previous memory allocation. For more such tips, feel free to refer my previous blog.
						</p>
						<kbd><b>!nvidia-smi</b></kbd>

						<p><strong>Kaggle Pipeline:
							</strong><br>I’m considering the Kaggle IEEE-CIS Fraud Detection competition, I’ll now breakdown step by step of a typical Kaggle machine learning pipeline in colab.
						<p><strong>1. Downloading the datasets from API calls:</strong><br>First download your API token by going to your Kaggle My Account (https://www.kaggle.com/*Your-Username*/account), going to section ‘API’ and clicking on ‘create new API token’. You will now download a file called ‘kaggle.json’. You have to upload this file to your colab notebook.</p>
						<kbd><b>
							from google.colab import files<br>
							
							uploaded = files.upload()</b>
						  </kbd>

						<p>You can use the code given below to download and unzip the datasets.</p>

						<kbd><b>!mkdir -p ~/.kaggle<br>
							!cp kaggle.json ~/.kaggle/<br>
							!chmod 600 ~/.kaggle/kaggle.json<br>
							!kaggle competitions download --force ieee-fraud-detection<br>
							#Unzip the downloaded files<br>
							!unzip sample_submission.csv.zip<br>
							!unzip test_identity.csv.zip<br>
							!unzip test_transaction.csv.zip<br>
							!unzip train_identity.csv.zip<br>
							!unzip train_transaction.csv.zip </b></kbd>						
						
							<p>You can now get access to the datasets of size ~1.2 GB in most efficient way.
							</p>

							


							<p><b>2. Pre-Processing and Data Wrangling:</b><br>Reading the datasets of this size will sometimes take a couple of minutes using pandas. To use the RAM and GPU provided effectively, we can use dask package to read these big datasets in less than a second!!
							</p>
							<kbd><b>import dask.dataframe as dd<br>
								train_identity = dd.read_csv('train_identity.csv')<br>
								train_transaction = dd.read_csv('train_transaction.csv')<br>
								test_identity = dd.read_csv('test_identity.csv')<br>
								test_transaction = dd.read_csv('test_transaction.csv')<br>
								sub = dd.read_csv('sample_submission.csv')<br>
								# let's combine the data and work with the whole dataset<br>
								train = dd.merge(train_transaction, train_identity, on='TransactionID', how='left')<br>
								test = dd.merge(test_transaction, test_identity, on='TransactionID', how='left')
								</b></kbd>	

							<p>Coming to data wrangling, I followed up by changing the necessary data types and train and test data splitting. This can be done according to their style and comfort. You can go through the <a href="https://colab.research.google.com/drive/1QK2Iw1At-sZuQbaBkOWAJZZKUhOVSyJ5">colab</a> for my steps for data wrangling.</p>
							
							<p><b>3. Feature Engineering and Feature Selection:</b><br>This can be done according to the type of datasets, taking freedom of creating new features and using feature selection techniques like Recursive Feature Elimination, BORUTA, Tree based feature selection, etc. I’ve used ‘sklearn feature_selection’ with the help of ‘SelectFromModel’ and using ‘XGBClassifier’ to do feature selection.
							</p>
							<kbd><b>
								from sklearn.feature_selection import SelectFromModel<br>
								from xgboost import XGBClassifier<br>

								xgbc=XGBClassifier(n_estimators=500,verbose=1,tree_method='gpu_hist')<br>

								embeded_xgb_selector = SelectFromModel(xgbc)<br>
								embeded_xgb_selector.fit(xTrain_xgb, yTrain_xgb)<br>

								embeded_xgb_support = embeded_xgb_selector.get_support()<br>
								embeded_xgb_feature = xTrain_lgb.loc[:,embeded_lgb_support].columns.tolist()<br>
								final_tr= xTrain_xgb[embeded_xgb_feature] </b></kbd>		
						</p>

						<p><b>4. Model building:</b>I’ve used XGboost to fit the data, parameters are selected using hyperparameter techniques like gridsearchCV or randomsearchCV or Bayesian optimization, Will be including in a detailed version of these hyperparameter optimization techniques in further blogs.
						</p>
						<p><kbd><b>import xgboost as xgb<br>
							model = xgb.XGBClassifier(
								n_estimators=500,
								max_depth=10,
								learning_rate=0.01,<br>&nbsp;&nbsp;
								subsample=0.9,
								colsample_bytree=0.9,
								random_state=123,<br>
								&nbsp;&nbsp;tree_method='gpu_hist' )<br>
							
							xgb_best=model.fit(final_tr, yTrain)
							</b></kbd></p>
						
						<p><strong>5. Submission File:</strong><br>After validating on the test data, further ‘submission data’ predictions of Kaggle can be added to the pipeline to make sure that the file is downloaded after the predictions.</p>
						<p><kbd><b>ytest_xgb=xgb_best.predict_proba(final_ts)<br>
							submission_xgb= np.c_[test.TransactionID,ytest_xgb[:,1]]<br>
							submission_xgb= pd.DataFrame(submission_xgb)<br>
							submission_xgb.columns= ['TransactionID','isFraud']<br>
							print(submission_xgb)<br>
							submission_xgb.to_csv('submission_xgb.csv')<br>
							from google.colab import files<br>
							files.download('submission_xgb.csv')</b></kbd></p>

						<p>Please check the detailed Kaggle pipeline from this <a href="https://colab.research.google.com/drive/1QK2Iw1At-sZuQbaBkOWAJZZKUhOVSyJ5">colab</a>, I’ve received a score of ~0.93 on the submission file with a minimal time of ~5 min to run the pipeline. we can then automate by changing the parameter grid and using different models like LightGBM, Catboost, Adaboost etc to improve the model.</p>
						<p><img class="center" src="images/works/kaggle1.png" alt="" /></p>
						<p><strong>PRO-TIP:</strong><br>Adding to this if you’re worried if the colab could be disconnecting you can run a small following Javascript code going to the console by clicking Ctrl+ Shift + i in your browser.</p>
						<p><kbd><b>function ClickConnect(){<br>
							console.log("Working"); <br>
							document.querySelector("colab-toolbar-button#connect").click()<br> 
							}setInterval(ClickConnect,60000)</b></kbd></p>
						<p><strong>Conclusion:</strong><br>Working on large datasets provided in the Kaggle competitions could be a time-taking process. If we can effectively make use of google colab notebooks to create a pipeline, we can then make use of parallel computing libraries like dask and use GPU effectively to accelerate and automate the process of Data modeling.</p>
						
						
						
						<p><strong>References</strong><br><a href="https://arxiv.org/abs/1603.02754">1. XGBoost.</a><br>
							<a href="https://www.kaggle.com/c/ieee-fraud-detection">2. Kaggle IEEE-CIS Fraud Detection competition.</a>
							<br>
						</p>

						<p>This article is originally published on medium publication <a href="https://towardsdatascience.com/automate-kaggle-competition-with-the-help-of-google-colab-4c43a6960115"><b>Towards Datascience</b></a></p>
					
					</div>
					<!--<div class="post-comments">
						<div class="title">
							<div class="title_inner">4 COMMENTS</div>
						</div>
						<ul class="comments">
							<li class="post-comment">
								<div class="image">
									<img src="images/man1.jpg" alt="">
								</div>
								<div class="desc">
									<div class="name">Jesse Pitman</div>
									<p>
										An has alterum nominavi. Nam at elitr veritus voluptaria. Cu eum regione tacimates vituperatoribus, ut mutat delenit est.
									</p>
								</div>
							</li>
							<li class="post-comment">
								<div class="image">
									<img src="images/man1.jpg" alt="">
								</div>
								<div class="desc">
									<div class="name">John Doe</div>
									<p>
										An has alterum nominavi. Nam at elitr veritus voluptaria. Cu eum regione tacimates vituperatoribus, ut mutat delenit est.
									</p>
								</div>
							</li>
							<li class="post-comment">
								<div class="image">
									<img src="images/man1.jpg" alt="">
								</div>
								<div class="desc">
									<div class="name">James Collins</div>
									<p>
										An has alterum nominavi. Nam at elitr veritus voluptaria. Cu eum regione tacimates vituperatoribus, ut mutat delenit est.
									</p>
								</div>
							</li>
							<li class="post-comment">
								<div class="image">
									<img src="images/man1.jpg" alt="">
								</div>
								<div class="desc">
									<div class="name">James Ferguson</div>
									<p>
										An has alterum nominavi. Nam at elitr veritus voluptaria. Cu eum regione tacimates vituperatoribus, ut mutat delenit est.
									</p>
								</div>
							</li>
						</ul>
						<div class="form-comment">
							<div class="title">
								<div class="title_inner">Write a comment</div>
							</div>
							<form id="comment_form" method="post">
								<div class="group-val">
									<input type="text" name="name" placeholder="Name" />
								</div>
								<div class="group-val">
									<input type="text" name="email" placeholder="Email" />
								</div>
								<div class="group-val ct-gr">
									<textarea name="message" placeholder="Comment"></textarea>
								</div>
								<a href="#" class="btn fill" onclick="$('#comment_form').submit(); return false;" data-text="Add Comment">Add Comment</a>
							</form>
						</div>
					</div>
					<div class="clear"></div>
				</div>
			</div>-->
		
		</div>
		
		<!-- Footer -->
		<footer>
			<div class="soc">
				<a target="_blank" href="https://www.linkedin.com/in/amaleshv/"><span class="ion ion-social-linkedin"></span></a>
				<a target="_blank" href="https://medium.com/@amalesh.v7"><span class="ion ion-outlet"></span></a>
				<a target="_blank" href="https://github.com/AmaleshV"><span class="ion ion-social-github"></span></a>
				<a target="_blank" href="https://www.instagram.com/me_amalesh/"><span class="ion ion-social-instagram-outline"></span></a>
				<a target="_blank" href="https://www.facebook.com/amalesh.vemula"><span class="ion ion-social-facebook-outline"></span></a>
			</div>
			<div class="copy">© Amalesh 2020. All rights reserved.</div>
			<div class="clr"></div>
		</footer>
		
		<!-- Lines -->
		<div class="line top"></div>
		<div class="line bottom"></div>
		<div class="line left"></div>
		<div class="line right"></div>
		
	</div>
		
		<!-- Lines -->
		<div class="line top"></div>
		<div class="line bottom"></div>
		<div class="line left"></div>
		<div class="line right"></div>
		
	</div>
	
	<!-- jQuery Scripts -->
    <script src="js/jquery.min.js"></script>
	<script src="js/jquery.validate.js"></script>
	<script src="js/typed.js"></script>
	<script src="js/magnific-popup.js"></script>
	<script src="js/masonry.pkgd.js"></script>
	<script src="js/imagesloaded.pkgd.js"></script>
	<script src="js/masonry-filter.js"></script>
	
	<!-- Main Scripts -->
	<script src="js/glitche-scripts.js"></script>
	
</body>
</html>